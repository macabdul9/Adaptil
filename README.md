# Adaptil
Investigating the Domain Robustness of Distilled Models and Pre-training models. 
Most of the experiment in this repo was presented in [Pretrained Transformers Improve Out-of-Distribution Robustness](https://arxiv.org/abs/2004.06100).

Tasks and Domains:
- Sentiment Analysis (IMDB, SST2)
- Natural Language Inference (MNLI Matching Domains/MNLI-HANS)
- Paraphase Indentification(QQP, PAWS)

Results:

